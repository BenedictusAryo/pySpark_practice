{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Environment Check and Sanity Test\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Import pyspark` check Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Example of PySpark Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/03/28 14:51:05 WARN Utils: Your hostname, Thnkpad-Yog370 resolves to a loopback address: 127.0.1.1; using 172.17.157.33 instead (on interface eth2)\n",
      "20/03/28 14:51:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/03/28 14:51:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/03/28 14:51:08 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/03/28 14:51:08 INFO SparkContext: Submitted application: Spark Pi\n",
      "20/03/28 14:51:08 INFO SecurityManager: Changing view acls to: benedict\n",
      "20/03/28 14:51:08 INFO SecurityManager: Changing modify acls to: benedict\n",
      "20/03/28 14:51:08 INFO SecurityManager: Changing view acls groups to: \n",
      "20/03/28 14:51:08 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/03/28 14:51:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(benedict); groups with view permissions: Set(); users  with modify permissions: Set(benedict); groups with modify permissions: Set()\n",
      "20/03/28 14:51:09 INFO Utils: Successfully started service 'sparkDriver' on port 51550.\n",
      "20/03/28 14:51:09 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/03/28 14:51:09 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/03/28 14:51:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/03/28 14:51:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/03/28 14:51:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ed189035-9a3b-431a-a553-668f81ee826a\n",
      "20/03/28 14:51:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/03/28 14:51:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/03/28 14:51:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/03/28 14:51:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.157.33:4040\n",
      "20/03/28 14:51:10 INFO SparkContext: Added JAR file:///home/benedict/hadoop/spark-2.4.5-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar at spark://172.17.157.33:51550/jars/scopt_2.11-3.7.0.jar with timestamp 1585381870530\n",
      "20/03/28 14:51:10 INFO SparkContext: Added JAR file:///home/benedict/hadoop/spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar at spark://172.17.157.33:51550/jars/spark-examples_2.11-2.4.5.jar with timestamp 1585381870532\n",
      "20/03/28 14:51:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/03/28 14:51:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51551.\n",
      "20/03/28 14:51:10 INFO NettyBlockTransferService: Server created on 172.17.157.33:51551\n",
      "20/03/28 14:51:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/03/28 14:51:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.157.33, 51551, None)\n",
      "20/03/28 14:51:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.157.33:51551 with 366.3 MB RAM, BlockManagerId(driver, 172.17.157.33, 51551, None)\n",
      "20/03/28 14:51:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.157.33, 51551, None)\n",
      "20/03/28 14:51:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.157.33, 51551, None)\n",
      "20/03/28 14:51:12 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "20/03/28 14:51:12 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "20/03/28 14:51:12 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "20/03/28 14:51:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/03/28 14:51:12 INFO DAGScheduler: Missing parents: List()\n",
      "20/03/28 14:51:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "20/03/28 14:51:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.0 KB, free 366.3 MB)\n",
      "20/03/28 14:51:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1381.0 B, free 366.3 MB)\n",
      "20/03/28 14:51:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.157.33:51551 (size: 1381.0 B, free: 366.3 MB)\n",
      "20/03/28 14:51:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163\n",
      "20/03/28 14:51:13 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "20/03/28 14:51:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 10 tasks\n",
      "20/03/28 14:51:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:13 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:13 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:13 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "20/03/28 14:51:13 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "20/03/28 14:51:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "20/03/28 14:51:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/03/28 14:51:13 INFO Executor: Fetching spark://172.17.157.33:51550/jars/spark-examples_2.11-2.4.5.jar with timestamp 1585381870532\n",
      "20/03/28 14:51:13 INFO TransportClientFactory: Successfully created connection to /172.17.157.33:51550 after 110 ms (0 ms spent in bootstraps)\n",
      "20/03/28 14:51:13 INFO Utils: Fetching spark://172.17.157.33:51550/jars/spark-examples_2.11-2.4.5.jar to /tmp/spark-8009806f-ce81-44cb-8b10-f0da3fde515c/userFiles-19ab17f3-64b5-4b32-88bb-ee117a59f5c5/fetchFileTemp2136762233919829355.tmp\n",
      "20/03/28 14:51:13 INFO Executor: Adding file:/tmp/spark-8009806f-ce81-44cb-8b10-f0da3fde515c/userFiles-19ab17f3-64b5-4b32-88bb-ee117a59f5c5/spark-examples_2.11-2.4.5.jar to class loader\n",
      "20/03/28 14:51:13 INFO Executor: Fetching spark://172.17.157.33:51550/jars/scopt_2.11-3.7.0.jar with timestamp 1585381870530\n",
      "20/03/28 14:51:13 INFO Utils: Fetching spark://172.17.157.33:51550/jars/scopt_2.11-3.7.0.jar to /tmp/spark-8009806f-ce81-44cb-8b10-f0da3fde515c/userFiles-19ab17f3-64b5-4b32-88bb-ee117a59f5c5/fetchFileTemp1409881465461331466.tmp\n",
      "20/03/28 14:51:13 INFO Executor: Adding file:/tmp/spark-8009806f-ce81-44cb-8b10-f0da3fde515c/userFiles-19ab17f3-64b5-4b32-88bb-ee117a59f5c5/scopt_2.11-3.7.0.jar to class loader\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 867 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 737 ms on localhost (executor driver) (1/10)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 826 ms on localhost (executor driver) (2/10)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 769 ms on localhost (executor driver) (3/10)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 770 ms on localhost (executor driver) (4/10)\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 106 ms on localhost (executor driver) (5/10)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 781 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, PROCESS_LOCAL, 7866 bytes)\n",
      "20/03/28 14:51:14 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 136 ms on localhost (executor driver) (6/10)\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 140 ms on localhost (executor driver) (7/10)\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 781 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 146 ms on localhost (executor driver) (8/10)\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 781 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 824 bytes result sent to driver\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 59 ms on localhost (executor driver) (9/10)\n",
      "20/03/28 14:51:14 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 75 ms on localhost (executor driver) (10/10)\n",
      "20/03/28 14:51:14 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 1.424 s\n",
      "20/03/28 14:51:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/03/28 14:51:14 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 1.602594 s\n",
      "Pi is roughly 3.1416871416871417\n",
      "20/03/28 14:51:14 INFO SparkUI: Stopped Spark web UI at http://172.17.157.33:4040\n",
      "20/03/28 14:51:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/03/28 14:51:14 INFO MemoryStore: MemoryStore cleared\n",
      "20/03/28 14:51:14 INFO BlockManager: BlockManager stopped\n",
      "20/03/28 14:51:14 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/03/28 14:51:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/03/28 14:51:14 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/03/28 14:51:14 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/03/28 14:51:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-00fe587f-a35c-40c3-8484-9d9b0c045007\n",
      "20/03/28 14:51:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-8009806f-ce81-44cb-8b10-f0da3fde515c\n"
     ]
    }
   ],
   "source": [
    "!run-example SparkPi 10\n",
    "\n",
    "# Or can also:\n",
    "# !spark-submit --master local[2] pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Setup Success"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
